---
layout: page
permalink: /projects/index.html
title: AntGLM-Med-10B
---


## AntGLM-Med-10B for PubMedQA 
<br> This project is affiliated with Ant Group, ranking <font color='red'>Top-5</font> on [PubMedQA leaderborad](https://pubmedqa.github.io/).

- [From Beginner to Expert: Modeling Medical Knowledge into General LLMs](https://Haishen-ll.github.io/file/AntGLM-Med.pdf)

- **Author Information**<br>
Qiang Li, mangxiao.lq@antgroup.com (Co-first Author)<br>
XiaoYan Yang, joyce.yxy@antgroup.com (Co-first Author)<br>
HaoWen Wang, wanghaowen.whw@antgroup.com<br>
Qin Wang, chengzhao.wq@antgroup.com<br>
Lei Liu, haishen.ll@antgroup.com  (Corresponding Author)<br>
Junjie Wang, benge.wjj@antgroup.com<br>
Yang Zhang, yaoling.zy@antgroup.com<br>
Mingyuan Chu, chumingyuan.cmy@antgroup.com<br>
Sen Hu, hs272483@antgroup.com<br>
Yicheng Chen, yicheng.chen@antgroup.com<br>
Yue Shen, zhanying@antgroup.com<br>
Cong Fan fancong.fan@antgroup.com<br>
Wangshu Zhang, wangshu.zws@antgroup.com<br>
Teng Xu, harvey.xt@antgroup.com<br>
JinJie Gu, jinjie.gujj@antgroup.com<br>
Jing Zheng, jing.zheng@antgroup.com<br>
GuanNan Zhang, zgn138592@antgroup.com<br>


- **Model Information**<br>
  Model Name: AntGLM-Med<br>
  Model Size: 10B<br>
  Accuracy: 80.6<br>
  Affiliation: Ant Group (Please note this information!)<br>



- ðŸš€ The main contrinbution of AntGLM-Med-10B is summarized as: we proposed a 3-stage optimization procedure for adpating a general LLM in the medical domain, including continue pre-training, instruction tuning, and task adpatation. The base model is based on AntGLM-10B, which is a general LLM developed by Ant Group. The AntGLM is featured by 2048 max length, 48 layers, a hidden size of 4096, 64 attention heads, and a prefix-decoder arcitecture. Other technical details can refer to the above-mentioned paper.

